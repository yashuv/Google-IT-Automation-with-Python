# Slowness
In this module, you’ll learn about what factors can cause a machine or program to act slowly. You’ll dive into ways of addressing slowness by identifying the bottleneck that might be causing the slowness. You’ll learn about tools to identify which resources are being exhausted, including iotop, iftop, and activity monitor in MacOS. Next, you’ll learn how computers use resources, and understand the differences between CPU, RAM, and Cache in order to help you find the possible causes for slowness in our machines or scripts. Next up, you’ll learn how to write efficient code, then explore profilers to help you identify where your code is spending most of its time. Next, you’ll dive into data structures and understand which ones are right for you to use. These include lists, tuples, dictionaries, sets, and expensive loops. Then, you’ll dive into complex slowness problems and how utilizing concurrency and adding a caching service can improve the execution of your code. Finally, you’ll understand how using threads can make the execution of your code much quicker.

## Learning Objectives
* Understand what slowness is and utilize tools to identify the bottleneck causing the issue
* Utilize tools like iotop and iftop to identify exhausted resources
* Understand the different computer components and how they can contribute to slowness
* Understand how to write efficient code, and utilize the use of data structures and loops to help your code run efficiently
* Utilize concurrency, caching services, and threads to improve the execution of your code

## Understanding Slowness:
### Intro to Module 2: Slowness
A problem that we have to deal with a lot when working in IT, is things being slow. This could be our computer, our scripts, or even complex systems. Slow is a relative term. Modern computers are much faster and can do many more things than computers a couple of decades ago. Still, we always want them to be faster and to do more in less time. With modern day computers it might seem that our resources are unlimited, but if we try hard enough we can hit the limits. For example, if you're browsing the internet and you open a new tab in your browser, it might seem like it doesn't take any resources at all. It's just a click and you have a new tab available. But if you keep opening tabs, at some point your computer will become sluggish. Depending on the hardware of the computer and what other programs are running, it might take 100 or more open tabs to get there. But eventually, you'll run out of memory and everything will slow down. There's a bunch of different things we can do if our system is too slow. The most obvious one is closing any applications we don't need at the moment. This works because it helps us free some of the resources in our computer, like CPU time, RAM, or video memory. That way the program that we want to run faster will have access to more of these resources. When closing applications that we don't need we might even need to look at applets, plugins, extensions, or other small programs that might seem harmless, as they take some resources to run. On top of that, closing any other elements that take resources, like browser tabs or open files in a document editor, can also help. But this only gets us so far because there's a ton of other reasons why our devices or programs might be slow. Over the course of the next few sections we'll do a rundown of the different reasons that can make things run slowly. We'll look into what causes slow scripts, slow computers, or slow systems. We'll give you the tools to help you identify the most common causes of slowness, and apply solutions to improve the overall performance.

### Why is my computer slow?
Our computers execute thousands of millions of instructions per second. Each of those instructions does one small thing, like incrementing a value, comparing two values, or moving a value from one place to another. Still with thousands of millions of instructions per second, there's a lot that a computer can do in just one second. This allows our computer to seemingly execute a number of different thing at the same time. Say you're browsing the web while also running an application that plays your favorite music in the background. Even if your computer has a single core to execute those applications, it will seem like the computer is running these two programs at the same time. What's happening under the hood is that each application gets a fraction of the CPU time, and then the next application gets a turn. Most of the time this works fine. But if you run too many applications or if one of these applications were running needs more CPU time than the fraction it's getting, things might become frustratingly slow. The general strategy for addressing slowness is to identify the [inaudible] for addressing slowness in our device, our script, or our system to run slowly. The bottleneck could be the CPU time as we just mentioned. But it could also be time spent reading data from disk waiting for data transmitted over the network, moving data from disk to RAM, or some other resource that's limiting the overall performance. Pretty often, we can speed things up and getting rid of anything else that's using resources on the same computer. So if the problem is that your program needs more CPU time, you can close other running programs that you don't need right then. If the problem is that you don't have enough space on disk, you can uninstall applications that you don't use, or delete or move data that doesn't need to be on that disk. If the problem is that the application needs more network bandwidth, you can try stopping any other processes that are also using the network and so on. This only helps us if the issue is that there are too many processes trying to use the same resource. If we've closed everything that wasn't needed and the computer is still slow, we need to look into other possible explanations. What if the hardware we're using just isn't good enough for the applications we're trying to run on it? In cases like these, will have to upgrade the underlying hardware. But to make a difference in the resulting performance, we need to make sure that we're actually improving the bottleneck and not just wasting our money on new hardware that will go unused. So how can we tell which piece of hardware needs to be changed? We need to monitor the usage of our resources to know which of them as being exhausted. This means that it's being used completely and programs are getting blocked by not having access to more of it. Is it the CPU, the memory, the disk IO, the network connection, the graphics card? To find out, we use the tools available in our operating system, monitor the usage of each resource, and then work out which one is blocking our programs for running faster. We've already talked about using top on Linux systems. This tool lets us see which currently running processes are using the most CPU time. If we start by memory, which ones are using the most memory. It also shows a bunch of other load information related to the current state of the computer, like how many processes are running and how the CPU time or memory is being used. We also called out in earlier sections a couple of other programs like `iotop` and `iftop`. They can help us see which processes are currently using the most disk IO usage or the most network bandwidth. On MacOS, the OS ships with a tool called Activity Monitor which lets us see what's using the most CPU, memory, energy, disk, or network. On Windows, there's a couple of OS tools called Resource Monitor and Performance Monitor which also let us analyze what's going on with the different resources on the computer including CPU, memory, disk and network. So if you're looking to diagnose what's causing your computer to run slow, the first step is always to open one of these tools. Check out what's going on, and try to understand which resources the bottleneck and why. Then plan how you're going to solve the issue. Of course, not all performance problems are solved by closing applications are getting better hardware. Sometimes, we need to figure out what the software is doing wrong and where it's spending most of its time to understand how to make it run faster. We need to really study each problem to get to the root cause of the slowness. Up next, we'll talk about some ideas to help us better understand what's going on under the hood.

### How Computers Use Resources
Previously, we called out how a computer can be constrained by different resources like CPU, disk, memory, or network. We discussed how we need to eliminate the bottlenecks and get our computer to better use its resources to boost our system's performance. To do that, we need to understand how each component interacts with the other and what the limitations are. In particular, when thinking about making things faster, it's important to understand the different speeds of the parts involved. When an application is accessing some data, the time spent retrieving that data will depend on where it's located. If it's a variable that's currently being used in a function, the data will be in the CPU's internal memory, and our program will retrieve it really fast. If the data is related to a running program but maybe not the currently executing function, it will likely be in RAM, and our program will still get to a pretty fast. If the data is in a file, our program will need to read it from disk, which is much slower than reading it from RAM, and worse than reading from disk, is reading information from over the network. In this case, we have a lower transmission speed, and we also need to establish the connection to the other endpoint to make the transmission possible, which adds to the total time needed to get to the data. So if you have a process that requires repeatedly reading data over the network, you might want to figure out if you can read it once stored on disk, and then read it from disk afterwards. Or similarly, if you repeatedly reading files from disk, you might see if you can put the same information directly into the process memory and avoid loading it from disk every time. In other words, you want to consider if you can create a cache, a cache stores data in a form that's faster to access than its original form. There's a ton of examples of caches in IT. A web proxy is a form of cache. It stores websites, images, or videos that are accessed often by users behind the proxy. So they don't need to be downloaded from the Internet every time. DNS services usually implement a local cache for the websites they resolve. So they don't need to query from the Internet every time someone asks for their IP address. The operating system also takes care of some caching for us. It tries to keep as much information as possible in RAM so that we can access it fast. This includes the contents of files or libraries that are accessed often, even if they aren't in use right now. We say that these contents are cached in memory. We call that that if the data is part of a program that's currently running, it will be in RAM. But RAM is limited. If you run enough programs at the same time, you'll fill it up and run out of space. What happens when you run out of RAM? At first, the OS will just remove from RAM anything that's cached, but not strictly necessary. If there's still not enough RAM after that, the operating system will put the parts of the memory that aren't currently in use onto the hard drive in a space called swap. Reading and writing from disk is much slower than reading and writing from RAM. So when the swapped out memory is requested by an application, it will take a while to load it back. The swapping implementation varies across the different operating systems, but the concept is always the same. The information that's not needed right now is removed from RAM and put onto the disk, while the information that's needed now is put into RAM. This is normal operation, and most of the time, we don't notice it. But if the available memory is significantly less than what the running applications need, the OS will have to keep swapping out the data that's not in use right now to move the data currently in use to RAM, and as we called out, our computer can switch between applications very quickly, which means that the data currently in use can also change very quickly. The computer will start spending a lot of time writing to disc to make some space in RAM and then reading from disk to put other things in RAM. This can be super slow. So what do you do if you find that your machine is slow because it's spending a lot of time swapping? There are basically three possible reasons for this. We've already talked about two of them. First, if there are too many open applications and some can be closed, close the ones that aren't needed. Or if the available memory is just too small for the amount that computer is using, add more RAM to the computer. The third reason is that one of the running programs may have a memory leak, causing it to take all the available memory. A memory leak means that memory which is no longer needed is not getting released. We'll talk a bunch more about memory leaks later in the course. For now, let's just say that if a program is using a lot of memory and this stops when you restart the program, it's probably because of a memory leak. Up next, we'll discuss a bunch more different reasons our computer might run slowly and what we can do to fix them.

### Possible Causes of Slowness
We've talked about a few different things that can make our computer slow. But of course, there's a lot more possible reasons. In this section, we'll do a rundown of some of the most common ones that you might come across in your IT role. Before we do, a quick reminder that when trying to diagnose why a computer is slow, we should use the process of elimination that we looked at earlier. We first look for the simplest explanations that are the easiest to check. And after eliminating a possible root cause, we go back to the problem and come up with the next possible cause to check. So when trying to figure out what's making a computer slow, the first step is to look into when the computer is slow. If it's slow when starting up, it's probably a sign that there are too many applications configured to start on boot.

In this case, fixing the problem is just a question of going through the list of programs that start automatically and disabling any that aren't really needed. If instead the computer becomes sluggish after days of running just fine, and the problem goes away with a reboot, it means that there's a program that's keeping some state while running that's causing the computer to slow down. For example, this can happen if a program stores some data in memory and the data keeps growing over time, without deleting old values. If a program like this stays running for many days, the data might grow so much that reading it becomes slow and the computer runs out of RAM. This is almost certainly a bug in the program. And the ideal solution for a problem like this is to change the code so that it frees up some of the memory used. If you don't have access to the code, another option is to schedule a regular restart to mitigate both the slow program and your computer running out of RAM. A similar problem that can trigger after a long time using an application, and that isn't solved by a reboot, is that the files that an application is handling have grown too large. So when the program needs to read those files, it gets really slow. Again, this generally points to a bug in the way the program was designed because it didn't expect the files to grow so large. The best solution in this case is to fix the bug. But what can you do if you can't modify the code of the program? You can try to reduce the size of the files involved. If the file is a log file, you can use a program like logrotate to do this for you. For other formats, you might need to write your own tool to rotate the contents. Another data point that we can use to diagnose what's going on is whether this happens for all users of the application or just a subset of them. If only some users are affected, we'll want to know if there's something that's configured differently on those computers that might be triggering the slowness. For example, many operating systems include a feature that tracks the files in our computer so it's easy and fast to search for them. This feature can be really useful when looking for something on a computer, but can get in the way of everyday use if we have tons of files and not the most powerful hardware. We've called out before that reading from the network is notably slower than reading from disk. It's common for computers in an office network to use a file system that's mounted over the network so they can share files across computers. This normally works just fine, but can make some programs really slow if they're doing a lot of reads and writes on this network-mounted file system. To fix this, we'll need to make sure that the directory used by the program to read and write most of its data is a directory local to the computer. Hardware failures can also cause our computer to become slow. If your hard drive has errors, the computer might still be able to apply error correction to get the data that it needs, but it will affect the overall performance. And once a hard drive starts having errors, it's only a matter of time until they're bad enough that data starts getting lost, so it's worth keeping an eye out for them. To do this, we can use some of the OS utilities that diagnose problems on hard drives or on RAM, and check if there's anything that could be causing problems. Yet another source of slowness is malicious software. Of course, we always want to keep your computer clean of any malicious software, but we can feel the effects of malicious software even if they aren't installed. For example, you might have come across a website that includes scripts, either in the website's content or the ads displayed, that use our processor to mine for cryptocurrency. Malicious browser extensions also fall into this category.

As you can see, there's a lot of possible reasons that could cause our computer to run slowly. Whenever we have to fix an issue like this, we need to look at what the bottleneck is, figure out the root cause behind the resource being used up, and then take appropriate action. Up next, we'll do a practical exercise of figuring out why our computer is slow and solving the issue.

### Slow Web Server
A user has alerted us that one of the web servers in our company is being slow, and we need to figure out what's going on. Let's start by navigating to the website and loading the page.

Okay. We see that the page loads. It seems to be a little slow but it's hard to measure this on our own. Let's use a tool called `ab` which stands for `Apache Benchmark` tool to figure out how slow it is. We'll run `ab -n 500` to get the average timing of 500 requests, and then pass our `site.example.com` for the measurement. i.e: `ab -n 500 site.example.com`. This tool is super useful for checking if a website is behaving as expected or not. It will make a bunch of requests and summarize the results once it's done. Here, were asking for it to do 500 requests to our website. There are a lot more options that we could pass like how many requests we want the program to do at the same time, or if the test to finish after timeout, even if not all requests completed, we're making 500 requests so that we can get an average of how long things are taking. Once the test finishes, we can look at the data and decide if it's actually slow or not. All right. The tool has finished running the 500 requests. We see that the mean time per requests was a 155 milliseconds. While this is not a super huge number, it's definitely more than what we'd expect for such a simple website. It seems that something is going on with the web server and we need to investigate further. Let's connect to the web server and check out what's going on.

We'll start by looking at the output of top and see if there's anything suspicious there.

We see that there's a bunch of `ffmpeg` processes running, which are basically using all the available CPU. See those load numbers? Thirty is definitely not normal. Remember that the load average on Linux shows how much time the processor is busy at a given minute with one meaning it was busy for the whole minute.

This is a CPU intensive process and seems like the likely culprit for our server being overloaded. So what can we do? 
One thing we can try is to change the processes priorities so that the web server takes precedence.
```bash 
for pid in $(pidof ffmpeg); do renice 19 $pid; done
```
Again run benchmark to see if it work out.
```bash 
ab -n 500 site.example.com
```
Again not solved, let's find more about the process and what triggered the process:
```bash 
ps ax | less          #shows all running process and connect o/p to less) to be able to scroll through process
/ffmpeg               #search process
locate <location>     #locate the location of .webm file
cd <location>         #change directory to located
ls -l                 #info about files
grep ffmpeg *         #find only ffmpeg
vim <filename.sh>     #make necessary changes where necessary
killall -STOP ffmpeg  #send stop signal but don't kill completely
```
Run process on at a time using automation instead of manually:
```bash 
for pid in $(pidof ffmpeg); do while kill -CONT $pid; do sleep 1; done; done
```
This may solve the slowness. Run benchmark once again and see if the mean time is now much lower than previous one.
```bash 
ab -n 500 site.example.com
```

This is just one sample example of slowness.

Next, we'll talk about how to improve performance by fixing our code. But before that, there's a reading to put all the resources mentioned in one place

### Monitoring Tools
Check out the following links for more information:

* https://docs.microsoft.com/en-us/sysinternals/downloads/procmon 

* http://www.brendangregg.com/linuxperf.html

* http://brendangregg.com/usemethod.html

* <a href="https://support.apple.com/en-us/HT201464">Activity Monitor in Mac</a>

* <a href="https://www.windowscentral.com/how-use-performance-monitor-windows-10">Performance Monitor on Windows</a> 

* https://www.digitalcitizen.life/how-use-resource-monitor-windows-7

* https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer

* https://en.wikipedia.org/wiki/Cache_(computing)

* https://www.reddit.com/r/linux/comments/d7hx2c/why_nice_levels_are_a_placebo_and_have_been_for_a/

## Slow Code:
### Writing Efficient Code
In your role as an IT specialist or systems' administrator, you'll likely need to write scripts to automate tasks. A piece of code may start as a simple script that does a single thing, but end up growing into a complex program that handles many different tasks, and no matter the size and complexity of our code, we usually want it to perform well. In this and the next few sections, we'll discuss some ideas for how to make our code more efficient and how to figure out what needs fixing if it's slow. One important thing to keep in mind though is that we should always start by writing clear code that does what it should and only try to make it faster if we realize that it's not fast enough. If it takes you 10 minutes to write a script that will run in five seconds, and 20 minutes to write a script that will do the same but takes three seconds, does it make a difference? It all depends on how often you run the script. If you run it once a day, the two seconds deference definitely won't justify the additional 10 minutes of work. But if you're going to run the same script for the 500 computers on your network, that small difference means it will take 15 less minutes to run the whole script. So overall, you're gaining time. Of course, it's pretty hard to know in advance how fast your script will be and how long it will take you to make it faster. But as a rule, we aim first to write code that's readable, easy to maintain and easy to understand, because that lets us write code with less bugs. If there's something that's super slow, then yes, it makes sense to fix it, particularly if the script will be executed frequently enough that making it faster will save you more time than the time you spend optimizing it. But remember, trying to optimize every second out of a script is probably not worth your time. Okay, with that said, let's dive into how we can make our code more efficient. The first step is to keep in mind that we can't really make our computer go faster. If we want our code to finish faster, we need to make our computer do less work, and to do this, we'll have to avoid doing work that isn't really needed. How? There's a bunch of different things to do. The most common ones include storing data that was already calculated to avoid calculating it again using the right data structures for the problem and reorganizing the code so that the computer can stay busy while waiting for information from slow sources like disk or over the network. To know what sources of slowness we need to address, we have to figure out where our code is spending most of its time. There's a bunch of tools that can help us with that called `profilers`. A `profiler` *is a tool that measures the resources that our code is using, giving us a better understanding of what's going on*. In particular, they help us see how the memory is allocated and how the time spent. Because of how profilers work, they are specific to each programming language. So we would use `gprof` to analyze a C program but use the `cProfile` module to analyze a Python program. Using tools like these, we can see which functions are called by our program, how many times each function was called and how much time are programs spent on each of them. This way we can find for example, that our program is calling a function more times than we originally intended or that a function that we thought would be fast is actually slow. To fix our code, we'll probably need to restructure it to avoid repeating expensive actions. What do we mean by expensive? In this context, expensive actions are those that take a long time to complete. Expensive operations include parsing a file, reading data over the network or iterating through a whole list. Okay. How do we modify our code to avoid expensive operations? We'll discuss a few strategies in our next section.

### Using the Right Data Structures
Having a good understanding of the data structures available to us can help us avoid unnecessary expensive operations and create efficient scripts. In particular, we'll want to understand the performance of those structures under different conditions. In the introductory course to Python, you learned about a bunch of different data structures available in Python like lists, tuples, dictionaries, and sets. Each of them have their uses, their advantages, and disadvantages. Let's do a very quick recap of lists and dictionaries. Lists are sequences of elements. We can add, remove, or modify the elements in them. We can iterate through the whole list to operate on each of the elements. Different programming languages call them differently. The structure is called ArrayList in Java, Vector in C++, Array in Ruby, and Slice in Go. All these names refer to the same data structure that's fast to add or remove elements at the end. But adding or removing elements in the middle can be slow because all the elements that follow need to be repositioned. It's fast to access the element in a specific position in the list, but finding an element in an unknown position requires going through the whole list. This can be super slow if the list is long. Dictionary store key value pairs. We add data by associating a value to a key. Then, we retrieve a value by looking up a specific key. They are called HashMap in Java, Unordered Map in C++, Hash in Ruby, and Map in Go. The map part in those names comes from how we're creating a mapping between a key and a value. The Hash part comes from the fact that to make the structure efficient, a hashing function is used internally to decide how the elements will be stored. The main characteristic of this structure is that it's super-fast for looking up keys. Once we have our data stored in a dictionary, we can find the value associated to a key in just one operation. If it were stored in a list, we need to iterate through the list. So as a rule of thumb, if you need to access elements by position or will always iterate through all the elements, use a list to store them. This could be a list of all computers in the network, of all employees in the company, or of all products currently on sale for example. On the flip side, if we need to look up the elements using a key, we'll use a dictionary. This could be the data associated to a user which we'd look up using their username, the IP associated to a computer using the host name, or the data associated to a product using the internal product code. Whenever we need to do a bunch of these lookup operations, creating a dictionary and using it to get the data will take a lot less time than iterating over a list to find what we're looking for. But it doesn't make sense to create a dictionary and fill it with data if we're only going to look up one value in it. In that case, we're wasting time creating the structure when we could just iterate over the list and get the element we're looking for. Another thing that we might want to think twice about is creating copies of the structures that we have in memory. If these structures are big, it can be pretty expensive to create those copies. So we should double-check if the copy is really needed. All right. Now, that we have a better understanding of when to use each data structure and what actions to avoid, we can look into how to deal with expensive loops. That's coming up in our next section.

### Expensive Loops
Loops are what make our computers do things repeatedly. They are an extremely useful tool and let us avoid repetitive work, but we need to use them with caution. In particular, we need to think about what actions we're going to do inside the loop, and when possible, avoid doing expensive actions. If you do an expensive operation inside a loop, you multiply the time it takes to do the expensive operation by the amount of times you repeat the loop. Say for example that you're writing a script to send an email to all the employees at your company asking them to verify that their emergency contact information is still valid. To send this out, you'll have a loop that sends one email per employee. In the body of the email, you'll include the current emergency contact data. The interesting part is how you access the data inside the loop. If the data is stored in a file, your script will need to parse the file to fetch it. If the script reads the whole file for every user, you'll be wasting a lot of time parsing the file over and over unnecessarily. Instead, you could parse the file outside of the loop, put the information into a dictionary, and then use the dictionary to retrieve the data inside the loop. Whenever you have a loop in your code, make sure to check what actions you're doing, and see if there are operations you can take out of the loop to do them just once. Instead of making one network call for each element, make one call before the loop. Instead of reading from disk for each element, read the whole thing before the loop. Even if the operations done inside the loop aren't especially expensive, if we're going through a list of a thousand elements and we only need five out of them, we're wasting time on elements we don't need. Make sure that the list of elements that you're iterating through is only as long as you really need it to be. Let's say you're running an internal website. As part of the information the site shows, it displays a list of the last five users that logged in. In the code, the program keeps a list of all the users that have logged in since it last started, and when the program needs to display the five latest users, it goes through the whole list and finds out which of those are the five most recent. This wastes a lot of time. If the service has been running for a while, it can take really long to go through the whole list. Instead, you could modify the service to store the user access info in log files that can be read if necessary and only keep the last five logins in memory. Whenever a new user logs in, the oldest entry in the list gets discarded and a new one gets added. That way, the script doesn't need to go through the whole list every time it needs to display the five most recent users. Another thing to remember about loops is to break out of the loop once you found what you were looking for. In Python, we do this using the keyword break. Breaking out of loops means that as soon as the data we're looking for is found, our script can continue. Of course if the data is at the end of the list, then we need to go through the loop anyway. But when the data is at the beginning of the list and not at the end, it makes sense to have our code break early to make the script faster. Say you're writing a script that checks if a given username is within the list of authorized entities, and if it is, it grants them access to a particular resource. You can use a for loop to iterate through the list of entities. When the username is found, you can break out of the loop and continue the rest of the script. One last thing to keep in mind is that the right solution for one problem might not be right for a different problem. Say your service has a total of 20 users. In that case, it's okay to go over this list whenever you want to check something. It's short enough that you don't need to do any special optimization. But if your service has over a thousand users, you'll want to avoid going through that list unless absolutely necessary. If the service has hundreds of thousands of users, going through that list isn't even a possibility. Up next, we'll talk about what we can do when the expensive operations are already outside of our loops

### Keeping Local Results
In the last section, we talked about how to avoid having expensive operations inside our loops. So if we have to parse a file, we do it once before we call the loop instead of doing it for each element of the loop. But what if parsing the file is taking a lot of time even when it's done outside of the loop? Remember that to make our scripts get to their goal faster, we need to avoid having our computer do unnecessary work. So how can we avoid expensive operations like parsing a file, downloading data over the network, or going through a long list? If the script gets executed fairly regularly, it's common to create a local cache. In an earlier video, we said that a cache is a way of storing data in a form that's faster to access than its original form. So if we're parsing a large file and only keeping a few key pieces of information from it, we can create a cache to store only that information, or if we're getting some information over the network, we can keep a local copy of the file to avoid downloading it over and over again. Creating caches can be super useful to save us time and make our programs faster. But they're sometimes tricky to get right. We need to think about how often we're going to update the cache and what happens if the data in the cache is out of date. If we're looking for some long-term stats, we can generate the cache once per day, and it won't be a problem. This might be the case for data like how much memory was used on computers across the fleet over the last month? How many employees each department in a company has? Or how many units were sold of each product over the last quarter? But if we're trying to look at data where the value as of right now is super important, we either can't use a cache or it has to be very short-lived. This could be the case for monitoring the health of computers to alert when something crosses a threshold. Checking the stock levels to see if there's enough of a product to sell or seeing if a username already exists in the network when trying to create a new one. Sometimes, we can add a check to validate if we need to recalculate the cache or not. For example, if our cache is based on a file, we could store the modification date of that file when we calculated the cache. Then only recalculate the cache if the modification date of the file is newer than the one we had stored. If we don't have a way of checking if our cache is out of date or not, we'll need to add in logic to our program that tries to make a sensible decision. For that, we'll take into account how often we expect the data to change, how critical it is that the latest data is used, and how frequently the program that we're running will be executed. After taking all these factors into account, we might decide that the cache needs to be recreated once per day, once per hour, or even once per minute. Yes, even once per minute might make sense if you have a script that can get executed several times per minute and needs to do an expensive operation that can be cached. That way, only the first execution in a minute will spend time on this operation, the rest will be very fast. But the cache is never more than a minute out of date. Keep in mind that caches don't always need to be elaborate structures, storing lots of information with a complex timeout logic. Sometimes, they can be as simple as having a variable that stores a temporary result instead of calculating this result every time we need it. For example, say you're generating a report that prints how many users there are in each of the different groups in the network. Now, some of these groups may contain other groups in them and some groups may even be part of several groups. For example, the Java release engineers group would be part of the release engineers group and the Java developers group. How can we avoid counting unique users more than once if they show up in multiple groups? We can have a dictionary with the group as the key and the amount of users as the value. That way, we only need to count the members of a group once, and after that, just use the value in the dictionary. To sum all of this up, remember that you'll want to look for strategies that let you avoid doing expensive operations. First, check if these operations are needed at all. If they are, see if you can store the intermediate results to avoid repeating the expensive operation more than needed. Up next, we'll look into a practical example of how to deal with some code that's running slower than expected.

### Slow Script with Expensive Loop
We can measure the script speed using the `time` command. 
```bash
time ./script.py <expected input for script> 
```
When we call *time* it runs the command that we pass to it and prints how long it took to execute it. There's *three different values*. _Real_, _user_, and _sys_. 
* `Real` is the amount of actual time that it took to execute the command. This value is sometimes called `wall-clock time` because it's how much time a clock hanging on the wall would measure no matter what the computer's doing. 
* `User` is the time spent doing operations in the user space. 
* `Sys` is the time spent doing system level operations. The values of user and sys won't necessarily add up to the value of real because the computer might be busy with other processes.

We  can combine a `profiler` and a `profile visualizer` to figure out where our code is spending most of the time.
```bash
pprofile3 -f callgrind -o profile.out ./scripy.py <expected input for script>   # profile in given format and o/p file

kcachegrind profile.out  # visualize the profiler o/p
```
`"kcachegrind"` is a tool used to visualize the performance data generated by profiling tools, including the Linux profiler `"callgrind."` The `kcachegrind` tool is used for profile data visualization that, if we can insert some code into the program, can tell us how long execution of each function takes.

Finally, we can change our code to avoid doing inexpensive things, for eg: avoiding expensive loop over and over by storing the information in a dictionary and then accessing the dictionary instead. Up next there's a reading with more information about profiling and after that a practice quiz to check if this is all still making sense.

### More About Improving Our Code
Check out the following links for more information:

* https://en.wikipedia.org/wiki/Profiling_(computer_programming)

## When Slowness Problems Get Complex:
## Parallelizing Operations
We've called that a few times already, that reading information from disk or transferring it over the network is a slow operation. In typical scripts while this operation is going on, nothing else happens. The script is blocked, waiting for input or output while the CPU sits idle. One way we can make this better is to do operations in parallel. That way, while the computer is waiting for the slow IO, other work can take place. The tricky part is dividing up the tasks so that we get the same result in the end. There's actually a whole field of computer science called concurrency, dedicated to how we write programs that do operations in parallel. We won't go into a ton of details here but we'll give you a brief overview of what you can do. First, we need to understand what the operating system already does for us. Our OS handles the many processes that run on our computer. If a computer has more than one core, the operating system can decide which processes get executed on which core, and no matter the split between cores, all of these processes will be executing in parallel. Each of them has its own memory allocation and does its own IO calls. The OS will decide what fraction of CPU time each process gets and switch between them as needed. So a very easy way to run operations in parallel is just to split them across different processes, calling your script many times each with a different input set, and just let the operating system handle the concurrency. Let's say you want to collect statistics on the current load and memory usage for all the computers in your network. You can do this by writing a script that connects to each computer in a list and gets the stats. Each connection takes a while to complete, so the total run-time of the script would be the sum of the time taken by all those connections. Instead, you could split the list of computers into smaller groups and use the OS to call the script many times once for each group. That way, the connections to the different computers can be started in parallel, which minimizes the time but the CPU isn't doing anything. This is super easy to do and for many scripts, it'll be the right choice. Another easy thing to do, is to have a good balance of different workloads that you run on a computer. If you have a process that's using a lot of CPU while a different process is using a lot of network IO and another process is using a lot of disk IO, these can all run in parallel without interfering with each other. When using the OS to split the work and the processes, these processes don't share any memory, and sometimes we might need to have some shared data. In that case, we'd use threads. Threads let us run parallel tasks inside a process. This allows threats to share some of the memory with other threads in the same process. Since this isn't handled by the OS, we'll need to modify our code to create and handle the threats. For that, we'll need to look into how the programming language we're using implements threading. In Python, we can use the Threading or AsyncIO modules to do this. These modules let us specify which parts of the code we want to run in separate threads or as separate asynchronous events, and how we want the results of each to be combined in the end. We won't go into the details of how you can do that here, but we'll link to more information on this in the next reading. One thing to watch out for is that depending on the actual threading implementation for the language you're using, it might happen that all threads get executed in the same CPU processor. In that case, if you want to use more processors, you'll need to split the code into fully separate processes. If your script is mostly just waiting on input or output, also known as I/O bound, it might matter if it's executed on one processor or eight. But you might be doing this in parallel because you're using all of the available CPU time. In other words, your script is CPU bound. In this case, you'll definitely want to split your execution across processors. Now there's a point where adding more parallel processes means things become even slower, not faster. If we're trying to read a bunch of files from disk and do too many operations in parallel, the disk might end up spending more time going from one position to another then actually retrieving the data, or if we're doing a ton of operations that use a lot of CPU, the OS could spend more time switching between them than actually making progress in the calculations we're trying to do. So when doing operations in parallel, we need to find the right balance of simultaneous actions that let our computers stay busy without starving our system for resources. I recently felt the benefits of applying concurrency. I was working on migrating data that was stored in one format, and I needed to store it in a different format. There were a lot of gigabytes of data that needed migrating, so of course I wasn't going to do it manually. My first version of the script was taking an average of one hour per gigabyte migrated. This was much slower than I expected, so I decided to spend more time tweaking the code to make the migration go faster. I reorganized the logic to have a separate thread per file which decreased the total time to work through the files since it now wasn't a linear process, and then, to make it go even faster, I split the work onto different machines, each running a bunch of threads. After all this rearranging to use the resources I have, I brought it down to three minutes per gigabyte. Yeah, we may be learning a lot about slowness, but we sure are moving fast. Some of these concepts may feel a little complex and it's perfectly normal. Take your time, everyone learns slowness at their own pace. Up next, we'll discuss the different approaches we can take to handle systems as they become more complex.

### Slowly Growing in Complexity
As we called out in an earlier section, a solution that's good for one problem might not be so good for a different problem. And as a system becomes more complex and grows in usage, a solution that worked well before may no longer be well-suited. Let's say you're writing a secret Santa script where each person gives a secret gift to one other randomly assigned person. The script randomly selects pairs of people and then sends an email to the gift-giver telling them who they're buying a present for. If you're doing this for the people working on your floor, you might just store the list of names and emails in a CSV file.

The file will be small enough that the time spent parsing it won't be significant. Now if this script grows into a larger project that handles everyone at your company and the company keeps hiring more and more people, at some point parsing the file will start taking a lot of time. This is where you might want to consider using a different technology.

For example, you could decide to store your data in a SQLite file. This is a lightweight database system that lets you query the information stored in the file without needing to run a database server. Using SQLite for the data probably works just fine for assigning secret Santas at your company. But imagine that you've kept adding features to the service. So it now includes a way to create a wish list, a machine learning algorithm that suggests possible gifts and a tracker that keeps a history of each present given. And since people at your company love the program so much, you've made it an external service available to anybody. Keeping all the data in one file would be too slow. So you'll need to move to a different solution. You have to use a fully-fledged database server. Probably even running on a separate machine than the one running the secret Santa service. And there's even one more step after that. If the service becomes really really popular, you might notice that your database isn't fast enough to serve all the queries being requested. In that case, you can add a caching service like `memcached` which keeps the most commonly used results in RAM to avoid querying the database unnecessarily. So we've gone from hosting the data in a CSV file to having it in a SQLite file then moving it to a database server and finally using a dynamic cacher in front of the database server to make it run even faster.

A similar progression can happen on the user facing side of the same project. Initially, we set the Santa service would simply send emails to the people on the list. That's fine if it's a small group and there's one person in charge of the script. But as the project grows more complex, you'd want to have a website for the service to let people do things like check who their assigned person is and create wish lists. Initially, this could just be running on a web server on the same machine as the data. If the website gets used a lot, you might need to add a caching service like Varnish. This would speed up the load of dynamically created pages. And eventually, this still might not be enough. So you need to distribute your service across many different computers and use a load balancer to distribute the requests. You could do this in-house with separate computers hosted at your company, but this means that as the application keeps growing you need to add more and more servers. It might be easier to use virtual machines running in the cloud that can be added or removed as the load sustained by the service changes.

These examples show how important it is to find the right solution for each problem. It makes no sense to deploy a multi server web service with a distributed database for storage when you're only going to have a few dozen users. You just need to pay attention to how the service is growing to know when you need to take the next step to make it work best for the current use case. Up next, we'll talk about some of the issues that you might come across when dealing with complex systems.

### Dealing with Complex Slow Systems
In the previous section, we discussed how systems that grow in usage also grow in complexity. In large complex systems, we have lots of different computers involved. Each one doing a part of the work and interacting with the others through the network. For example, think of an e-commerce site for your company. The web server is the part of the system that directly interacts with external users. Another component is the database server, which is accessed by the code that handles any requests generated from the website, and depending on how the whole system is built, you might have a bunch of other services involved doing different parts of the work. There could be a billing system that generates invoices once orders are placed. A fulfillment system used by the employees preparing the orders for customers. A reporting system that once a day creates a report of all the sales placed and possibly more. On top of this, you should probably have backup, monitoring, testing infrastructure, and so on. A system like this can be tricky to debug and understand. What do you do if your complex system is slow? As usual, what you want to do is find the bottleneck that's causing your infrastructure to underperform. Is it the generation of dynamic pages on the web server? Is it the queries to the database? Is it doing the calculations for the fulfillment process? Figuring this out can be tricky. So one key piece is to have a good monitoring infrastructure that lets you know where the system is spending the most time. Saying notice that getting the web pages is pretty slow. But when you check the web server, you see that it's not overloaded. Instead, most of the time is spent waiting on network calls, and when looking at your database server, you find that it's spending a lot of time on Disk I/O. This shows that there's a problem with how the data is being accessed in the database. One thing to look at is the indexes present in the database. When a database server needs to find data, it can do it much faster if there's an index on the field that you're querying for. On the flip side, if the database has too many indexes, adding or modifying entries can become really slow because all of the indexes need updating. So we need to look for a good balance of having indexes for the fields that are actually going to be used. If the problem is not solved by indexing and there are too many queries for the server to reply to all of them on time, you might need to look into either caching the queries or distributing the data to separate database servers. Now what if when you try to figure out why the service is slow, you see that the CPU on the web serving machine is saturated. The first step is to check if the code of the service can be improved using the techniques that we explained earlier. If it's a dynamic website, we might try adding caching on top of it. But if the code is fine and the cache doesn't help because the problem is that there's just too many requests coming in for one machine to answer all of them, you'll need to distribute the load across more computers. To make this possible, you might need to reorganize the code so that it's capable of running in a distributed system instead of on a single computer. This might take some work, but once you've done it, you can easily scale your application to as many requests as needed by adding more computers to the system, and finally, make sure that you actually need to do whatever you're doing. Lots of times, as projects evolve, we're left with a scary monster of layer after layer of complex code. If we think about what our system is doing for a few minutes, we might end up discovering that there's a whole piece that wasn't needed at all and it was making our servers do unnecessary work all along. If all of this is starting to sound too difficult and scary, don't worry. Remember that if you ever need to deal with such complex systems, one of your best tools is to ask your colleagues for help. Up next, we'll try our hand at solving a real life problem with something complex.

### Using Threads to Make Things Go Faster
 Our company has an e-commerce website that includes a bunch of images of the products that are up for sale. There's a rebranding coming up, which means that all of these images will need to be replaced with new ones. This includes both the full-size images and the thumbnails. We have a script that creates the thumbnails based on the full-size images. But there's a lot of files to process, and our script is taking a long time to finish. It looks like it's time to take it up a notch and use something better to do the resizing. We'll start by trying out the current script as-is using a set of 1,000 test images. There's more images to convert, but it'll be easier to test the speed of our script with a smaller batch. We'll execute our program using the time command to see how long it takes.

It took about two seconds for 1,000 images. This doesn't seem too slow, but there's tens of thousands of images that need converting, and we want to make sure that the process is as fast as possible. Let's try making this go faster by having it process the images in parallel. We'll start by importing the futures sub module, which is part of the concurrent module. This gives us a very simple way of using Python threads.

To be able to run things in parallel, we'll need to create an executor. This is the process that's in charge of distributing the work among the different workers. The futures module provides a couple of different executors, one for using threads and another for using processes. We'll go with the `ThreadPoolExecutor` for now.

Now the function that does most of the work in this loop is process_file. Instead of calling it directly in the loop, we'll submit a new task to the executor with the name of the function and its parameters.

Our for loop now creates a bunch of tasks that are all scheduled in the executor. The executor will run them in parallel using threads. An interesting thing that happens when we use threads is that the loop will finish as soon as all tasks are scheduled. But it will still take a while until the tasks complete. So we'll add a message saying that we're waiting for all threads to finish, and then call the shutdown function on the executor. This function waits until all the workers in the pool are done, and only then shuts down the executor.

All right, we've made the change, let's save our script and test it out.

Our script now takes 1.2 seconds. That's a nice improvement over the two seconds we saw before. See how the user time is higher than the real time? By using multiple threads, our script is making use of the different processors available in the computer. And this value shows the time used on all processors combined. What do you think will happen if we try to use processes instead of threads? Let's try this out by changing the executor that we're using.
Play video starting at :4:26 and follow transcript4:26
By changing the executor to the ProcessPoolExecutor, we tell the futures module that we want to use processes instead of threads for the parallel operations. Let's save and try this one out now.

Wow, this is now taking less than a second to finish, and the user time has gone up even more. This is because, by using processes, we're making even more use of the CPU. The difference is caused by the way threads and processes work in Python. Threads use a bunch of safety features to avoid having two threads that try to write to the same variable. And this means that when using threads, they may end up waiting for their turn to write to variables for a few milliseconds, adding up to the small difference between the two approaches.

In this section, we looked into how we can add threading support to a Python script to make better use of our processor power. There's still more improvements that we can make to our script, like checking if the thumbnail exists and is up to date before doing the conversion. Or adding a second progress bar while waiting for tasks to finish, to make it clear that our script is doing its job. We won't go into those here, but if you're interested, you can explore those possibilities on your own. Up next, another reading with pointers to more information.

### More About Complex Slow Systems
We only touched briefly on the ways we can use concurrency to improve our programs. If you're interested in learning more, <a href="https://realpython.com/python-concurrency/">this article</a> from Real Python has a lot of details on the different ways to use concurrency in Python.

Check out the following links for more information:

* https://realpython.com/python-concurrency/

* https://hackernoon.com/threaded-asynchronous-magic-and-how-to-wield-it-bba9ed602c32* 

### Conclusion

Over the last few section we've learned about the many different factors that can cause our computer to run slowly. We didn't cover absolutely every possible cause, but we did check out the most common reasons. We talked about how the first thing to do when faced with a slow system is to identify the bottleneck. To do this, you'll need to understand how each component interacts with the system and what resource is being exhausted. Sometimes the root cause is that the hardware isn't enough, or maybe it's just that there are too many things happening at the same time. Other times the problem might be in the code itself. When trying to fix a program that's slow, we should avoid code that does expensive operations. We went over several best practices to help us write better performing code. We discussed when to use the right data structures, how to avoid expensive loops, and how to keep results local, by creating a cache for example. We then spent some time reviewing complex systems and looked at some examples on how to troubleshoot slowness in this type of environment. Next time you have to debug a performance problem, you'll be able to think about what the bottleneck is, look for what's exhausting that resource, and come up with ideas for how to make things go faster. I hope you're starting to see how the skills you're picking up can really help you as you face some of these same issues at your workplace. There's no way you're going to be able to solve every issue, but hopefully you're starting to feel even more confident in your skills and abilities. 

Up next, in the another practical lab we'll have to debug a system that's running slow and make it perform better.

