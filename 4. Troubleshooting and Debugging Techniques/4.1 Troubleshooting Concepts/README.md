# Troubleshooting Concepts
In this module, you’ll be introduced to the fundamentals of troubleshooting and you’ll learn different strategies and approaches to tackle problems that you might encounter. You’ll learn about the concept of debugging and how debugging is one of the core principles of troubleshooting. You’ll be introduced to some tools that will help you in the debugging process, like tcpdump, ps, top, itrace, and lots more. Next, you’ll explore how to “understand the problem.” This might sound like a no brainer, but it's not as easy as you might think! Next, we’ll dive into the different approaches when troubleshooting reproducing errors versus intermittent errors. Finally, you’ll learn about “binary searching a problem.” We’ll explore the different types of searches, including binary and linear searches. Then, we’ll learn about the concept of bisecting and how it can be used in your troubleshooting approach, and finish up with finding invalid data in a CSV file.

## Learning Objectives
* Understand the concept of troubleshooting
* Grasp what debugging is and how it can help in the troubleshooting process
* Understand how to get to the root cause of a problem
* Utilize the right approach for recurring problems and intermittent problems
* Understand the difference between linear and binary searches
* Search CSV files for invalid data

## Intro to Debugging:
### Debugging vs Troubleshooting
So what's the difference between them? We say that troubleshooting is the process of identifying, analyzing, and solving problems. We can use the term troubleshooting to refer to solving any kind of problem. In this course, we'll focus on troubleshooting IT-related problems. They could be problems caused by hardware, the operating system, or applications running on the computer. They could also be caused by the environment and configuration of the software. The services the application is interacting with, or a wide range of other possible IT causes. On the flip side, debugging is the process of identifying, analyzing, and removing bugs in a system. We sometimes use troubleshooting and debugging interchangeably. But generally, we say troubleshooting when we're fixing problems in the system running the application, and debugging when we're fixing the bugs in the actual code of the application. 

### Problem Solving Steps
There's a set of steps that you can usually take to solve almost any technical problem. The three basic steps of problem-solving that they don't always happen sequentially are: 

**1. Getting information**
* Gathering as much information as we need about the current state of things, what the issue is, when it happens, and what the consequences are, for example.
*  To get this information, we can use any existing documentation that might help. This can be internal documentation, manual pages, or even questions asked on the Internet.
* One super important resource to solve a problem is the *reproduction case*, which is a clear description of how and when the problem appears.

**2. Finding the root cause**
* This is usually the most difficult step.
* The key here is to get to the bottom of what's going on, what triggered the problem, and how we can change that.

**3. Performing the necessary remediation**
* Depending on the problem, this might include an immediate remediation to get the system back to health, and then a medium or long-term remediation to avoid the problem in the future.

### Silently Crashing Application
Say a user contacts us to let us know that a certain application fails to open. As we called out earlier the first thing to do is to get more information about the conditions that caused the failure. What the error is that the user is getting and then check if we experience the same failure. By asking for these details, we discover that a new version of the software was recently released. And when we upgrade to this new version, we can reproduce the problem on our own computer like this.

We see that when we try to run the program it prints no error at all. It just exits immediately. We need to figure out what's going on. Even if there's no error message.

There are a bunch of tools that can help us better understand what's going on with the system and with our applications. With the help of these tools, we can extend our knowledge of a particular problem view the actions of the program from a different point of view and get the info we need. Among these tools `strace` lets us look more deeply at what the program is doing. It will trace a system calls made by the program and tell us what the result of each of these calls was. So to figure out what's up with our program that's failing to open will `strace` the failing application.

## Understanding the Problem:

### "It Doesn't Work"
As we called out, the first step to solving a problem is getting enough information so that we can understand the current state of things. To do this we'll need to know what the actual issue we're solving is. This starts when we first come across the issue, which can be through report by a ticketing system or by encountering the problem ourselves. When working with users, it's pretty common to receive reports of failures that just boil down to, `"It doesn't work`." These reports usually don't include a lot of useful information but it's still important that the problem gets reported and solved. Which information is useful or not might depend on the problem. But there are some common questions that we can ask a user that simply report something doesn't work. What were you trying to do? What steps did you follow? What was the expected result? What was the actual result? If the ticketing system your company uses allows this, it's a good idea to include these questions in the form that users have to fill out when reporting an issue. This way we save time and can start asking more specific questions right away. Otherwise, these are almost always going to be the first questions you ask. Another thing to keep in mind is that when debugging a problem, we want to consider the simplest explanations first and avoid jumping into complex or time-consuming solutions unless we really have to. That's why when a device doesn't turn on, we first check if it's correctly plugged in and that there's electricity coming from the plug before taking it apart or replacing it with a new device. Say you got a call from a user that tells you the internal website used by the sales team to track customer interactions doesn't work. The user is super stressed because they need to access the information on the website for a meeting happening in a few minutes. So you tell them that you'll look into the problem right away, but then you need more information. What were they trying to do? The user tells you that they're trying to access the website. What steps did they follow? They tell you that they opened the website URL and entered their credentials. What was the expected result? They expected to see the sales system's landing page. What did they get instead? The web page just keeps loading. It stays blank forever. Okay. So now you've gone from, "it doesn't work," to, "when I tried to log in, the page keeps loading and never shows the landing page." That's great. Now that you have a basic idea of what the problem is, it's time to start figuring out the root cause. For that, you'll apply a process of elimination, starting with the simplest explanations first and testing those until you can isolate the root cause. For example, you check if you can reproduce the issue on your own computer. So you navigate to the website, enter your credentials, and sure enough, the page just keeps loading, never showing the landing page. This is enough information that you can tell the user that you'll work on it and investigate on your own. There's no need to keep them on the line. By reproducing the problem on your computer, you've taken a simple and quick action that rules out the user or the user's computer as the cause of the problem. This cuts the troubleshooting process in half since you now know there's a problem with the service and you can focus on solving that. Before jumping into the server that's hosting the application, you run a few quick checks to verify if the problem is isolated to that specific website or not. You check if your Internet access is working successfully by accessing an external website which loads just fine. Then you check if other internal websites, like the inventory website or ticketing system are working okay. Doing this, you discover that while the ticketing system loads with no issues, the inventory website never finishes loading. It turns out both websites are hosted on the same server. Again, it's important to highlight that doing these quick checks to verify that the Internet works correctly and which sites are affected by the problem, helps you isolate the root cause. By looking at possible simple explanations first, you avoid losing time chasing the wrong problem. At this point, you know that website's running on a specific server or failing to load while the rest of the systems and the Internet are working correctly. Next up, you need to check what's going on on that server. The server running the websites is a Linux machine, so you'll connect to it using SSH. You run the top command which shows the state of the computer and processes using the most CPU and see that the computer is super overloaded. The load average in the first line says 40. The load average on Linux shows how much time a processor is busy in a given minute, with one meaning it was busy for the whole minute. So normally this number shouldn't be above the amount of processors in the computer. A number higher than the amount of processors means the computer is overloaded. You know this computer has four cores, so 40 is a really high number. You also see that most of the CPU time is spent in waiting. This means that processes are stuck waiting for the operating system to return from system calls. This usually happens when processes get stuck gathering data from the hard drive or the network. By looking at the list of processes, you realize that the backup system is currently running on the server, and it seems to be using a lot of processing time. Backing up the data on the system is super important. But currently, the whole system is unusable. So you decide to stop the backup system by calling kill-stop. This will suspend the execution of the program until you let it continue or decide to terminate it. After doing this, you're on top once again and you see that the load is going down, and so processes are no longer stuck waiting for I/O. Then you try logging into the website, and this time the landing page loads. Success. You let the user know that they can use the website once again. At this point, you've applied the immediate remediation. We'll talk about long-term remediation in a later section.

Before moving on to the next topic, imagine that the following week another user calls you and tells you the sales website doesn't work. Remembering the previous incident, you tell them you'll fix it right away. You SSH onto the server and try to find the backup process to stop it, but it's not running. Oops. You forgot to ask the user what they meant when they said it didn't work. When you call back to ask them they tell you that they're trying to generate a monthly sales report and they get an error saying the product category column doesn't exist. Totally different problem, totally different actions to take. So remember to always have a clear picture of what the problem is before you start solving it. Up next, we'll talk about what are reproduction cases and how to come up with it.

### Creating a Reproduction Case
When we're dealing with an issue that's tricky to debug, we want to have a clear reproduction case for the problem. `A reproduction case is a way to verify if the problem is present or not.` We want to make the reproduction case as simple as possible. That way, we can clearly understand when it happens, and it makes it really easy to check if the problem is fixed or not, when we try to solve it. Sometimes, the reproduction case is pretty obvious. In our example where the program fail to start because of a missing directory, the reproduction case was to open the program without that directory on the computer. On our overloaded server example, the reproduction case for the failure was to try to login to the website and see the loading page. But sometimes the reproduction case might be much more complex to discover. Imagine you're trying to help a user with an application that won't start. This time when you run the same version of the application on your computer, the application starts just fine. So you suspect that the problem has to do with something in the user's environment or configuration. There could be a bunch of reasons why this could happen. It could be problems with the network routing, old config files interfering with a new version of the program, a permissions problem blocking the user from accessing some required resource, or even some faulty piece of hardware acting out. So how can you figure out what's causing the problem. The first step is to read the logs available to you. Which logs to read, will depend on the operating system and the application that you're trying to debug. On _Linux_, you'd read system logs like `/var/log/syslog` and user-specific logs like the `.xsession-errors` file located in the user's home directory. On _MacOs_, on top of the system logs, you'd go through the logs stored in the `library/logs` directory. On _Windows_, you'd use the `Event Viewer tool` to go through the event logs. No matter the operating system, remember to look at the logs when something isn't behaving as it should. Lots of times, you'll find an error message that will help you understand what's going on like, unable to reach server, invalid file format, or permission denied. But what if you're unlucky, and there's no error message, or the error message is super unhelpful like internal system error. The next step is to try to isolate the conditions that trigger the issue. Do other users in the same office also experienced the problem? Does the same thing happen if the same user logs into a different computer? Does the problem happen if the applications config directory is moved away? Let's say that it's the config directories file. You ask the user to move it away without deleting it, and now the application starts correctly. So you ask the user to send you the contents of that directory. You copy them onto your computer, and the program fails to start. Bingo, you got your reproduction case. It's starting the program with that config in place. Having a clear reproduction case, let's do investigate the issue, and quickly see what changes it. For example, does the problem go away if you revert the application to the previous version? Are there any differences in the strace log, or the ltrace logs when running the application with the bug config and without it? On top of that, having a clear reproduction case, lets you share with others when asking for help. As long as you aren't sharing any confidential information of course, you could use it to report a bug to the applications developers, to ask for help from a colleague, or even to ask for help from an Internet forum about the application if it's publicly available. So when trying to create a reproduction case, we want to find the actions that reproduce the issue, and we want these to be as simple as possible. The smaller the change in the environment and the shorter the list of steps to follow, the better. To get there, we might need to dig deeper into the problem until we have a small enough set of instructions. Once you have a reproduction case, you're ready to move on to the next step, `finding the root cause`.

### Finding the Root Cause
When you first come across these concepts, it might seem that once you have a reproduction case, you already know the root cause of the problem. But more often than not, it's not true. In our overloaded server example, we figured out that the backup system was blocking the websites from working, and so we mitigated that immediate problem to unblock the user. But we didn't really look into the root cause of our server being stuck. This could be because the network bandwidth is saturated, the disk transfer is too slow, the hard drive is faulty, or a bunch of other reasons. We also didn't do anything to make sure our backups could run successfully in the future. Understanding the root cause is essential for performing the long-term remediation. So how do we go about finding the actual root cause of the problem? We generally follow a cycle of looking at the information we have, coming up with a hypothesis that could explain the problem, and then testing our hypothesis. If we confirm our theory, we found the root cause. If we don't, then we go back to the beginning and try different possibility. This is where our problem-solving creativity comes into play. We need to come up with an idea of a possible cause, check if it's correct and if not, come up with a different idea until we find one that explains the problem. Our ideas don't come out of a void. To get inspired, we look at information we currently have and gather more if we need. Searching online for the error messages that we get or looking at the documentation of the applications involved can also help us imagine new possibilities of what might be at fault. Whenever possible, we should check our hypothesis in a test environment, instead of the production environment that our users are working with. That way, we avoid interfering with what our users are doing and we can tinker around without fear of breaking something important. Depending on what you're trying to fix, this might mean we have to try our code in a newly installed machine, spinning up a test server, using test data, and so on. It can take some time to get the setup, but the extra safety is definitely worth it. Even when it seems that the error might be related to the specific production environment, it's always a good idea to check if we can reproduce the problem in a test environment before we modify production. In our overloaded server example, if the problem is with the hardware, we wouldn't be able to replicate it with a test server. In that case, we would need to either wait until the services aren't being used or bring up a secondary server, migrate the services there, and only then look at what's wrong with the computer. On the flip side, if the problem is related to some configuration of either the web services or the backup service, we'd still see it in the test server. So we'd always start by setting up a test instance of the service and checking if the problem replicates there before touching the production instance. So say we have a test server running the same websites. When we start the backup, we see that the website stop responding. This is great because we have re-production case, and we can debug it properly. How do we find the root cause? One possible culprit could be too much disk input and output. To get more info on this, we could use iotop, which is a tool similar to top that lets us see which processes are using the most input and output. Other related tools are iostat and vmstat, these tools show statistics on the input/output operations and the virtual memory operations. If the issue is that the process generates too much input or output, we could use a command like ionice to make our backup system reduce its priority to access the disk and let the web services use it too. What if the input and output is not the issue? Another option would be that the service is using too much network because it's transmitting the data to be backed up to a central server and that transmission blocks everything else. We can check this using iftop, yet another tool similar to top that shows the current traffic on the network interfaces. If the backup is eating all the network bandwidth, we could look at the documentation for the backup software and check if it already includes an option to limit the bandwidth. The rsync command, which is often used for backing up data, includes a -bwlimit, just for this purpose. If that option isn't available, we can use a program like Trickle to limit the bandwidth being used. But what if the network isn't the issue either? Remember, we need to put our debugging creativity to work, and come up with other possible reasons for why it's failing. Another option could be that the compression algorithms selected is too aggressive, and compressing the backups is using all of the server's processing power. We could solve this by reducing the compression level or using the nice command to reduce the priority of the process and accessing the CPU. If that's still not the case, we need to keep looking, check the logs to see if we find anything that we missed before. Maybe look online for other people dealing with similar problems related to interactions of the backing up software with the web surfing software, and keep doing this until we come up with something that could be causing our problem. This sounds like a lot of work, but it's usually not that bad. In general, by using the tools available to us, we can find enough info to land on the right hypothesis after only a few tries and with experience, we'll get better at picking up the most likely hypothesis the first time around. Up next, we'll talk about a tricky type of technical problem that we all have to face, `intermittent issues`.

### Dealing with Intermittent Issues
Have you ever tried to solve a problem that happened only occasionally? Maybe you've dealt with programs that randomly crash, laptops that sometimes fail to suspend, web services that unexpectedly stop replying, or file contents that get corrupted. But only in some cases, bugs that come and go are hard to reproduce, and are extremely annoying to debug. If you work in IT, you've probably had your own dose of frustration while dealing with intermittent and issues. So what can you do if you're trying to debug an issue like that? The first step is to get more involved in what's going on, so that you understand when the issue happens and when it doesn't. If you're dealing with a bug and a piece of code that you maintain, you can usually modify the program to log more information related to the problem. Since you don't know exactly when the bug will trigger, you need to be thorough with the information that you log. For example, I recently had an issue with the service IAM. It was crashing sporadically, and I was at a loss trying to find out why. Looking at the error message, I knew it had something to do with strings that use special characters, but I had no idea where the bug was exactly. So I added more logging information to the service, around the inputs and the function calls that I suspect could be involved. The next time the program crashed, I was able to identify the part of the code where I was missing the proper handling for the encoding, and fixed the problem. If you can't modify the code of the program to get more information, check if there's a logging configuration that you can change. Many applications and services already include a debugging mode that generates a lot more output then the default mode. By enabling the debug information in advance, you can get a better picture of what's going on the next time the problem happens. If that's not possible, you'll need to resort to monitoring the environment when the issue triggers. Depending on what the problem is, you might want to look at different sources of information, like the load on the computer, the processes running at the same time, the usage of the network, and so on. For bugs that occur at random times, we need to repair our system to give us as much information as possible when the bug happens. This might require several iterations until we get enough information to understand the issue, but don't lose hope. Most of the time, you can finally get to the point where you can actually understand what's going on. Sometimes, the bug goes away when we add extra logging information, or when we follow the code step by step using a debugger. This is an especially annoying type of intermittent issue, nicknamed Heisenbug, in honor of Werner Heisenberg. He's the scientist that first described the observer effect, where just observing a phenomenon alters the phenomenon. Heisenbugs are extra hard to understand, because when we meddle with them, the bug goes away. These bugs usually point to bad resource management. Maybe the memory was wrongly allocated, the network connections weren't correctly initialized, or the open files weren't properly handled. In these cases, we usually need to just spend time looking at the effected code until we finally figure out what's up. Yet another type of intermittent issue is the one that goes away when we turn something off and on again. There's plenty of jokes related to how, in IT, a lot of what we do to solve problems, is just turn things off and on again. Okay, it's true that in many cases, power cycling a device or restarting a program gets rid of whichever problem we were trying to fix. But why is that? When we reboot a computer or restart a program, a bunch of things change. Going back to a clean slate means releasing all allocated memory, deleting temporary files, resetting the running state of programs, re-establishing network connections, closing open files and more. If a problem goes away by turning it off and on again, there's almost certainly a bug in the software, and the bug probably has to do with not managing resources correctly. So if an issue goes away after a restart, it's a good idea to try to figure out why that is, and see if it's possible to fix it in a way that doesn't require turning it off and on again. If in the end, we can't find the actual reason, scheduling a restart at a time that's not problematic can also be an option. So we've looked at a few ways of getting to the root cause of a problem, like isolating causes, understanding error messages, adding logging information, and generating new ideas for possible failures. We've also talked about problems that go away on their own and then pop up again, and looked at how to figure those out. Coming up, we'll check out a practical example of understanding a problem, and how to find its root cause.

### Intermittently Failing Script
A colleague recently developed a small application to send meeting reminders to people in the company, because someone kept forgetting to show up. The sales team was the first to test the app last week, and it worked fine. But this week, another user is trying to send a meeting reminder and the program keeps terminating with an error. Since the colleague that developed the app is on the other side of the Atlantic, the user is asking for our help to figure out what's going on. First, let's try running the program ourselves and check if we can reproduce the problem.

We're presented with a window where we can enter the date for the meeting, the title of the meeting, and the people that we want to send the reminder to. The meeting reminder that the user was trying to send was for January 13th, and the title was Production Review. As we are trying things out and don't want to be mailing reminders out with our tests, I'll set this one to send the reminders to meet.

It failed to send the email. This means we've reproduced the issue. Let's try reminder that the sales team sent last week which had worked fine. In that case, the reminder had been sent for January 7th and the title was Sales All Hands. Again, I'll send this to myself to avoid spanning people would test reminders.

Yes. In this case, the program successfully sent the reminder. Which parameter do you think is at fault? The title or the date? It could be either. But I'll bet it's the date. Let's try it once more with January 13th as the date and Sales All Hands as the title.

Another failure. So we have a reproduction case. When we try to send the meeting reminder for January 13th, we get the failure message. But if we try to send the same reminder for January 7th, it works fine. Now, the next step is to find the root cause of the issue. Why could our application work fine for January 7th but fail for January 13th? There could be a bunch of reasons. But in general, when dates are involved in a failure, the problem is due to how the dates are formatted. In some countries, the dates are written with the month first and the day second. While in other countries, it's the other way around. To figure out what's going on, let's add more debugging information to the program. We'll open the meeting_reminder.sh script, which is a script written in Bash.
Play video starting at :3:8 and follow transcript3:08
We see that this script is calling a program called Zenity. Zenity is the application showing the window to select the date, title, and emails. The output generated by Zenity is stored in a variable called meeting_info, which is then passed as a parameter to the send_reminders.py, Python 3 script. This script then sends the emails. To get more information about the output generated by Zenity, we'd like to see the value of the meeting_info variable before the Python script gets called. Let's add an echo statement to see that.

Let's save this and try again. This time, we'll just use test as the meeting title, as we know the problem is with the date.
Play video starting at :4:26 and follow transcript4:26
We see that the information generated by Zenity is split by pipes, and that the data is formatted as month, day, year. That's already valuable information. Now, the next step is getting a more informative error. To do that, let's open the Python script that sends the reminders, and see if we can make it print a better error.
Play video starting at :5: and follow transcript5:00
The file is long, so it makes sense to start by looking at the main function that lists the core functionality of the program. We see that it splits the parameter receive in three, then prepares the message to be sent, and finally sends it. If everything works fine, it prints a message saying that it was sent successfully. But if anything fails, it prints the error message that we've seen already. But the error message is not very useful, as it's hiding the reasons why things failed. Let's make this error more helpful by also printing the exception that generated the failure.

Let's save and try again.
This time we see that the problem is that the date format we are using is putting the month first, but the program is expecting to have the month second. As there's no month 13, this is an invalid date. So we found the root cause of the problem. The program is trying to convert the date assuming one specific date format, but we're using a different format. As we now know, once we know the root cause, the next step is to remediate the issue. What can we do in this case to remediate the problem? We could change the program to use our date format, but then the application would break for people running it in a different location. What we need to do is make sure that no matter where we run the script, the date generated by Zenity matches the date expected by Python. Fortunately, Zenity includes a parameter to specify any format we want.

So we'll change the shell script to use the -- forms-date-format parameter and set the format to %y -%m-%d, which is the international standard date format.
Play video starting at :7:13 and follow transcript7:13
With that, Zenity will return the date in the international format. Now, we need to change the Python script to use the same format. We'll go to the function that has the format specified and change it to the same format.
Play video starting at :7:58 and follow transcript7:58
Great. Now, the date format generated by Zenity should always match the one read by Python. This script should work in our country and any others. Let's try it out and check if it's really fixed.
Play video starting at :8:26 and follow transcript8:26
We've successfully fixed the issue. Let's quickly recap what we did. We first reproduced the problem ourselves, then found which input triggered the issue and which one didn't. We then added more debugging information to the scripts, which helped us find the root cause of the problem, a mismatch between the date formats used by this Zenity invocation and the Python script. Finally, we fixed the issue by making sure that both were using the same date format. 

## Binary Searching a Problem:

### What is binary search?
Usually when trying to find the root cause of a problem, we'll be looking for one answer in a list of many. Searching for an element in a list is a common problem in computing. There are a bunch of different algorithms that can help us find the element that we're looking for. Say for example, you have a list that contains the data of employees that work at your company and you want to find one specific employee. One possible approach would be to start from the first entry and then check if the name is the one that we're looking for. If it doesn't match, move to the second element and check again, and keep going until we find the employee with the name we're looking for, or we get to the end of the list. This is called a linear search. This type of search works but the longer the list, the longer it can take. In other words, the time it takes to find the result is proportional to the length of the list. If the list is sorted, we can use an alternative algorithm for searching called binary search. Because the list is sorted, we can make decisions about the position of the elements in the list. So the first thing we do is compare the name that we're looking for with the element in the middle of the list and check if it's equal, smaller, or bigger. If it's smaller, we know that the element we're looking for must be in the first half of the list. On the flip side, if it's bigger, we know that it's in the second half of the list. This way, with only one comparison, we've eliminated half of the list from possible candidates where the element could have been found, and then we do the same thing again and again until we find the element. So if the element we were looking for was smaller than the middle element, we look at the element in the middle of the first half. If our element is now bigger, we look at the element in the middle of the second quarter, and so on. Each time we look at the middle element of the section we're dealing with, until we find the element we're looking for. Using linear search, going through a list with 1000 elements might take up to 1,000 comparisons if the element we're looking for is the last one in the list or isn't present at all. Using binary search for the same list of 1,000 elements, the worst-case is only 10 comparisons. This is calculated as the base two logarithm of the lists length, and the benefits get more and more significant the longer the list. For a list of 100,000 elements, it would be 17 comparisons instead of 100,000 comparisons. But remember, that for this to work, the list needs to be sorted. So if the list isn't sorted, we would need a sort it first, which takes a chunk of time. It can still make sense to do it if we're going to search through it several times but it doesn't make sense to sort the list and then use binary search to only find one element. In that case, using linear search is simpler and faster. If you're curious about what these two types of search look like when implemented in Python, you can see a possible implementation of linear search and one for binary search in the next reading. After that, we'll talk about how we can apply the principles of binary search to troubleshooting.

### Linear and Binary Search (Optional)
If you're curious about how linear and binary search look in code, here are a couple of implementations in Python:

```python
def linear_search(list, key):
    """If key is in the list returns its position in the list,
       otherwise returns -1."""
    for i, item in enumerate(list):
        if item == key:
            return i
    return -1
```
```python
def binary_search(list, key):
    """Returns the position of key in the list if found, -1 otherwise.

    List must be sorted.
    """
    left = 0
    right = len(list) - 1
    while left <= right:
        middle = (left + right) // 2
        
        if list[middle] == key:
            return middle
        if list[middle] > key:
            right = middle - 1
        if list[middle] < key:
            left = middle + 1
    return -1
```

Don't worry if this seems complex! Understanding this code isn’t required for understanding how to use binary search in troubleshooting.

### Applying Binary Search in Troubleshooting
We call that the binary search algorithm is really efficient when trying to find an element in a sorted list. In troubleshooting, we can apply this idea when we need to go through and test a long list of hypotheses. When doing this, the list of elements contains all the possible causes of the problem and we keep reducing the problem by half until only one option is left. The list of elements could be entries in a file, extensions enabled, boards connected to a server, or even lines of code added to a faulty release and with each iteration, the problem is cut in half. This approach is sometimes called `bisecting` which means dividing in two. In an earlier video, we gave the example of a new version of a program that fail to start when the old configuration directory was present. If the directory contained a bunch of different files in it, we could identify the one causing the failure by bisecting the list of files. Say the old directory contained 12 different config files. We want to identify which of those 12 is causing the failure. To do that, we can create a copy of the directory with just six of the 12 files and then try to start the program again. If it crashes, then the bad file is among those six files. If it doesn't, it's among the other six. In the next step, we would pick three out of the failing group of six. If the program crashes again, it's one of those three. If it doesn't, it's one of the other three. For the last three, we can first check two together or just go one by one. Either way, it's two checks to get to the failing file. This means that with a total of four attempts, we can find out which of the 12 files is causing the problem. Since things in IT can sometimes be complex and intertwined, before declaring victory, we want to verify that the program crashes with that single file present and doesn't crash when the single file isn't present. Once we've confirmed that, we've reduced the reproduction case of our problem to a single file instead of a whole directory much easier to understand and figure out what's going on. After that, we can proceed in the same way with the contents of that single file, cutting it in half repeatedly, until we find the specific part of the file that's causing the problem. The same process can be applied to a large variety of problems. It's very common, for example to use it to figure out which browser extension is causing the browser to crash, disabling half of the extensions then checking if the browser crashes with that subset and so on until we find the faulty extension. We can also use this technique to discover which plug-in in a desktop environment is causing the computer to run out of memory, or which entry in a database is causing the program to raise an exception. We can also apply this to code when trying to find a bug that was introduced in a recent version. If we know the list of changes that were made between one version and the next, we can keep cutting that list in half until we find the one that caused the failure. When using Git for version control, we can use a Git command called `bisect`. Bisect receives two points in time in the Git history and repeatedly lets us try the code at the middle point between them until we find the commit that caused the breakage. This doesn't even need to be your Git repository. If you're using open source software that's tracking Git, you can use the bisect command to find out which commit cause the software to stop working on your computer. For example, if the latest release of the Linux kernel causes the sound card on your computer to stop working, you can use `git bisect` to find the commit that broke it and report this as a bug to be fixed. As we called out when we were talking about binary search, the longer the list of items that needs to be checked, the more we'll gain by cutting our problem in half on each iteration. If it's just five options that need to be checked, we can simply go one-by-one. It won't make a lot of difference and it might be easier to keep track of what we tried. But if it's a 100, we definitely want to bisect the problem so we can find the answer in seven steps. Not a 100. When we have to test a bunch of different options to find the one that's causing a failure, we'll want a quick and easy way to check it. Even if we're reducing the amount of attempts by bisecting the problem, we don't want to spend a long time on each check. Sometimes it's straightforward. Either the program starts or it fails. But other times, it can take a bunch of manual steps to check what we want to check. So depending on what the problem is that we're trying to find, it might make sense to spend some time creating a script that checks for the issue. Up next, we'll see this in action in a practical example.

### Finding Invalid Data
We have discussed how we can quickly find out the reason for a problem in a list of possible reasons by splitting the problem in half and testing each half separately. Let's see this in action with an example. We have a program that reads data from a CSV file, processes it, and then imports it into a database. One of the users of the system tells us that the file they're trying to import fails with an obscure import error. They've sent us the file so we can try it ourselves. To call the command, we'll connect the output of cat contacts.csv, the file that the user sent us, to the import.py command. But before we run the command, it's a good time to remember that we shouldn't test in production. And since this script is going to be trying to import data into a database, we should run it against the test database instead of the production database. To do that, we'll use the --server flag that takes the name of the database server, and then we'll pass the test as the parameter.
* cat contacts.csv | ./import.py --server test

## Module 1 Review:
We learned the general principles of debugging and troubleshooting. We looked into the basic process of solving a technical problem like *getting information*, *finding the root cause*, and *implementing the remediation*. We learned about a bunch of different tools and techniques that we can use to better understand what's going on with our systems and our programs, including how to create a reproduction case, how to find the root cause for problem, and how to deal with issues that only appear occasionally. Finally, we learned about the binary search algorithm, and how we can use it to bisect a problem and quickly find the root cause of a technical problem. All along, we've checked out a bunch of real-world examples, and seen how we can apply this to lots of different types of problems, like a bug in our code, a bug in someone else's code, a configuration issue, or even a hardware problem. It's been great learning all the interesting stories and examples with instructor. I am starting to have fun learning more about how to understand problems and find solutions. Next time when I need to solve a technical problem, I will try to use some of these steps we outlined and the ideas we talked about. I won't forget that logs are our best friend, and use all the resources available to me, including looking things up on the Internet, and asking colleagues or friends for help. 

Throughout the rest of this course, we'll keep exploring scenarios that deal with specific problems, like our computer being slow or crushing unexpectedly. We'll keep applying the techniques we've explained in this module to solve those issues.
